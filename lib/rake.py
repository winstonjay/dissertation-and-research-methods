# -*- coding: utf-8 -*-
'''rake.py:
Derivation of R.A.K.E (rapid automatic keyword extraction) algorithm as
described in:
Rose, S. Engel, D. Cramer, N. and Cowley, W. 2010.
Automatic keyword extraction from individual documents
https://goo.gl/KjkEbf

NOTE: This algorithm needs stopwords and punctuation to be present
within the data in order for it to work.
'''
from __future__ import division
from __future__ import print_function

import re
import collections
import operator as op


class KeywordExtractor(object):
    '''KeywordExtractor class implements an methods derived from the RAKE
    (Rapid automatic Keyword Extraction) algorithm to provide ways of quickly
    generating keywords from individual documents.'''
    def __init__(self, stop_words, phase_lim='.,?!:;'):
        self.stop_words  = stop_words
        self.stop_tokens = stop_words | set(phase_lim)
        # set word splitting pattern for extracting candidates
        pattern = '(?:[A-Z][\.-])+|\w+|[{}]'.format(re.escape(phase_lim))
        self.regex = re.compile(pattern)

    def keywords(self, document):
        'return a sorted list of keywords (keys only)'
        return [k for k, _ in self.extract_keywords(document)]

    def candidates(self, document):
        'return a set all posible candidates'
        return set(self.__extract_phrases((document)))

    def extract_keywords(self, document):
        'return a sorted list of (keyword, score) tuples'
        scores = self.all_scores(document)
        T = len(scores) // 3
        return sorted(scores, key=op.itemgetter(1), reverse=True)[:T]

    def all_scores(self, document):
        'return a list scores for all phrases'
        uniq = set()
        freq = collections.defaultdict(int)
        degs = collections.defaultdict(int)
        for phrase in self.__extract_phrases(document):
            uniq.add(phrase)
            words = phrase.split()
            for w in words:
                freq[w] += 1
                degs[w] += len(words)
        return [(k, self.__phrase_score(k, degs, freq)) for k in uniq]

    ####### keyword score calculation

    def __phrase_score(self, k, degs, freq):
        'compute sum of deg(w) / freq(w) for each word in a phrase'
        return sum((degs[w]) / freq[w] for w in k.split())

    def __extract_phrases(self, document):
        'return a generator that emits candidate phrases.'
        phrase = []
        for w in self.regex.findall(document.lower()):
            if w in self.stop_tokens:
                if phrase and is_valid(phrase):
                    yield concat(phrase)
                phrase = []
            else:
                phrase.append(w)
        if phrase and is_valid(phrase):
            yield concat(phrase)

    def adjoin(self, keywords):
        # TODO: look through the document to find keywords that should be
        # joined up. That is if a stopword has split them on more than one
        # occasion the example that is given is 'axis (of) evil'.
        # its score shall be the sum of its constituent scores.
        pass

def is_valid(phrase):
    'return if a phrase is a valid keyword'
    # TODO: This is a make do solution for now. probally change.
    text = concat(phrase)
    return (not text.isdigit() and
            text not in ignore_terms and
            (len(text) > 3 or text[0].isupper()))

concat = ' '.join

ignore_terms = ('day', 'year', 'last', 'years', 'first')



class CollectionOperator(object):
    '''CollectionOperator implements methods to describe a collection of
    documents. It takes as its main argument a keyword extractor which
    must implement the methods keywords and candidates.'''
    ## Finding the top keywords in the collection.
    # Some additional mesures described in the paper:
    #
    # refereneced document frequency: rdf(k) = how many documents a candidate
    # keyword has appeared in.
    #
    # extraction document frequency: edf(k) = how many documents a keyword has
    # been extracted from.
    #
    # exclusiveness: exc(k) = edf(k) / rdf(k)
    # essentialness: ess(k) = exc(k) * edf(k)
    # genrality:     gen(k) = rdf(k) * (1.0 - exc(k))
    #
    # the paper describes:
    # 'Keywords that are both highly essential and highly general are essential
    # to a set of documents within the corpus but also referenced by a
    # significantly greater number of documents within the corpus than other
    # keywords.'
    def __init__(self, extractor):
        # keyword extractor class.
        self.extractor = extractor
        # messures that can be calculated.
        self._general   = {}
        self._essential = {}
        self._exclusive = {}

    # NOTE prehaps vecorization would be better here the formulas described in
    # the paper all work with the features generated by reference document
    # frequency and extraction document freqency.
    def fit(self, docs):
        # refereneced document frequency
        rdf = collections.defaultdict(int)
        # extraction document frequency
        edf = collections.defaultdict(int)
        for doc in docs:
            for k in self.extractor.keywords(doc):
                edf[k] += 1
            for k in self.extractor.candidates(doc):
                rdf[k] += 1
        # populate all the feilds for each
        for k in rdf:
            self._exclusive[k] = edf[k] / rdf[k]
            self._essential[k] = self._exclusive[k] * edf[k]
            self._general[k]   = rdf[k] * (1.0 - self._exclusive[k])

    @property
    def exclusive(self):
        'return the most exclusive keywords. exc =  edf / rdf'
        if not self._exclusive:
            raise AttributeError('Collection must be fitted via self.fit()')
        return self._exclusive

    @property
    def essential(self):
        'return the most essential keywords. ess = exc * edf'
        if not self._essential:
            raise AttributeError('Collection must be fitted via self.fit()')
        return self._essential

    @property
    def general(self):
        'return the most general keywords. gen = rdf * (1 - exc)'
        if not self._general:
            raise AttributeError('Collection must be fitted via self.fit()')
        return self._general

    def add(self, document):
        'add document to the collection'
        self.docs.append(document)
